{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sources_path = './../Sources/'\n",
    "if sources_path not in sys.path:\n",
    "    sys.path.append(sources_path)\n",
    "# from adaptive_filtering.lms import LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = complex(0,1)\n",
    "n_ensembles = 100   # number of realizations within the ensemble\n",
    "K = 500             # number of iterations (signal length)\n",
    "H = np.array([0.32+0.21*j,-0.3+0.7*j,0.5-0.8*j,0.2+0.5*j])\n",
    "w_o = H            # Unknown system\n",
    "sigma_n2 = .04     # noise power\n",
    "N = 4              # Number of coefficients of the adaptive filter\n",
    "mu = .1            # Convergence factor (step) (0 < mu < 1)\n",
    "gamma = 1e-12      # small positive constant to avoid singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (np.sign(np.random.randn(K)) + j*np.sign(np.random.randn(K)))/np.sqrt(2) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        +0.j        ,  0.        +0.j        ,\n",
       "        0.        +0.j        ,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j, -0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678+0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j, -0.70710678-0.70710678j,\n",
       "        0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678-0.70710678j,\n",
       "       -0.70710678-0.70710678j,  0.70710678+0.70710678j,\n",
       "       -0.70710678-0.70710678j])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_extended = np.append(np.zeros([4-1]), x)\n",
    "x_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. +0.j 1. -1.j 0.5+1.j]\n",
      "[ 1. +1.j   2. +0.j  -0.5+1.5j]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1+0j, 1-1j, 0.5+1j])\n",
    "k = (1+1j)\n",
    "print (arr)\n",
    "print (k*arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_window(x_extended, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      " -0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [ 0.        +0.j          0.        +0.j         -0.70710678-0.70710678j\n",
      " -0.70710678+0.70710678j] \n",
      "\n",
      "(4,) [ 0.        +0.j         -0.70710678-0.70710678j -0.70710678+0.70710678j\n",
      " -0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [-0.70710678-0.70710678j -0.70710678+0.70710678j -0.70710678-0.70710678j\n",
      " -0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [-0.70710678+0.70710678j -0.70710678-0.70710678j -0.70710678-0.70710678j\n",
      "  0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [-0.70710678-0.70710678j -0.70710678-0.70710678j  0.70710678-0.70710678j\n",
      "  0.70710678+0.70710678j] \n",
      "\n",
      "(4,) [-0.70710678-0.70710678j  0.70710678-0.70710678j  0.70710678+0.70710678j\n",
      " -0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [ 0.70710678-0.70710678j  0.70710678+0.70710678j -0.70710678-0.70710678j\n",
      " -0.70710678+0.70710678j] \n",
      "\n",
      "(4,) [ 0.70710678+0.70710678j -0.70710678-0.70710678j -0.70710678+0.70710678j\n",
      " -0.70710678-0.70710678j] \n",
      "\n",
      "(4,) [-0.70710678-0.70710678j -0.70710678+0.70710678j -0.70710678-0.70710678j\n",
      "  0.70710678-0.70710678j] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for arr in rolling_window(x_extended, 4):\n",
    "    print (arr.shape, arr, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0,1,2,3,4,5,6,7,8,9])[1+4-1:1-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.41783981e-03, 2.17367582e-02, 2.23603776e-02, 2.47023157e-02,\n",
       "       4.98619218e-02, 2.46372470e-02, 1.25894053e-01, 9.32829672e-03,\n",
       "       2.36470856e-02, 2.17277918e-02, 2.28925683e-01, 1.10478064e-02,\n",
       "       3.11666478e-02, 1.78727923e-02, 6.54702903e-02, 4.51137647e-03,\n",
       "       9.17427326e-02, 4.68979274e-03, 2.53344149e-02, 1.03951816e-02,\n",
       "       2.42115446e-02, 1.00087080e-02, 2.00205649e-02, 6.66704900e-03,\n",
       "       6.07312190e-03, 9.08294640e-02, 1.12285789e-02, 5.64219547e-02,\n",
       "       1.93048076e-02, 2.47472581e-02, 5.46907254e-02, 2.10279501e-03,\n",
       "       5.58629840e-02, 2.22002535e-01, 3.04256081e-02, 1.12076665e-02,\n",
       "       1.95654377e-02, 8.03370595e-02, 2.94132321e-02, 5.02549319e-02,\n",
       "       3.01635940e-02, 3.65070578e-02, 1.81503147e-01, 1.18769273e-01,\n",
       "       7.38986248e-02, 3.98347886e-02, 2.69484022e-02, 2.49085369e-02,\n",
       "       3.27249606e-02, 7.25599569e-04, 3.83721471e-02, 7.00849509e-03,\n",
       "       1.45135067e-02, 8.69666718e-03, 6.61524474e-03, 1.26059765e-02,\n",
       "       2.38719963e-02, 2.11277045e-02, 4.58010853e-03, 2.14929704e-02,\n",
       "       1.55058506e-01, 7.63360837e-03, 1.77428741e-02, 2.79935366e-02,\n",
       "       2.98170243e-02, 1.63517764e-02, 1.09501039e-01, 8.61345634e-02,\n",
       "       7.38663586e-02, 7.67603793e-03, 3.44763327e-03, 9.54498066e-02,\n",
       "       7.54736033e-02, 9.19108964e-03, 7.85249493e-03, 2.56548380e-02,\n",
       "       1.98922242e-02, 1.73599565e-01, 3.15129938e-02, 6.26943373e-02,\n",
       "       3.36083927e-02, 5.74209426e-04, 5.24780476e-02, 2.00180843e-02,\n",
       "       1.61937255e-03, 2.76394132e-03, 2.05244100e-02, 9.36919729e-03,\n",
       "       6.91023310e-03, 9.70420106e-02, 8.34312285e-03, 6.18417570e-03,\n",
       "       3.35018590e-02, 2.44785822e-04, 6.37447349e-02, 5.43004626e-03,\n",
       "       5.76682540e-04, 3.29086167e-02, 1.97927242e-02, 2.68060897e-02,\n",
       "       6.80729630e-03, 1.07984174e-02, 3.75311636e-02, 3.34556595e-02,\n",
       "       7.48219099e-03, 5.14692869e-02, 4.05489400e-02, 7.07963771e-02,\n",
       "       3.11274698e-03, 3.47324257e-02, 9.51103028e-03, 4.34534203e-03,\n",
       "       1.15530433e-02, 1.02672876e-02, 4.39412110e-02, 1.02834609e-01,\n",
       "       1.79251520e-02, 1.04767262e-03, 3.71177138e-02, 3.98545422e-02,\n",
       "       3.01921287e-02, 7.57066579e-02, 5.52552589e-02, 3.50019327e-02,\n",
       "       7.66516134e-02, 4.46880558e-03, 9.06914169e-02, 2.07252847e-02,\n",
       "       7.94674158e-03, 1.74787994e-02, 7.04656558e-02, 6.01715737e-02,\n",
       "       2.28309756e-02, 5.35735841e-02, 4.82443625e-03, 7.93876318e-03,\n",
       "       3.78779517e-02, 3.87897897e-03, 1.82262308e-02, 2.57634469e-02,\n",
       "       4.83503730e-03, 6.86191478e-03, 1.79500984e-03, 1.89932640e-02,\n",
       "       8.53462168e-03, 8.31101402e-02, 7.66371179e-02, 3.35236332e-02,\n",
       "       6.04930336e-02, 1.87913127e-03, 1.86132628e-02, 6.28843166e-02,\n",
       "       4.91509653e-02, 2.70781644e-02, 3.08430039e-02, 5.56629243e-02,\n",
       "       5.99353000e-02, 2.24652103e-02, 5.37572096e-03, 1.91868230e-02,\n",
       "       1.29963798e-02, 3.04732700e-03, 7.18170867e-02, 4.98288385e-02,\n",
       "       6.65069576e-02, 3.09570997e-02, 7.58351473e-02, 5.67120242e-02,\n",
       "       1.93238372e-02, 2.28423866e-02, 2.84925708e-02, 6.73884543e-02,\n",
       "       7.04000209e-03, 7.93305267e-02, 6.61679181e-03, 5.44233597e-02,\n",
       "       2.79493858e-02, 1.23823131e-02, 6.94698527e-02, 3.09010252e-02,\n",
       "       2.02385970e-03, 1.32793244e-01, 1.73141753e-02, 3.18515298e-02,\n",
       "       2.28677775e-02, 8.08996894e-02, 6.73250064e-03, 8.94844483e-02,\n",
       "       2.30889837e-02, 1.13936885e-03, 9.51712356e-03, 6.20393338e-02,\n",
       "       1.70781026e-01, 8.64208004e-02, 1.59293113e-03, 4.90525758e-03,\n",
       "       1.90983445e-01, 6.48457562e-03, 2.87277411e-02, 7.76537016e-02,\n",
       "       9.66633688e-02, 5.07200958e-02, 8.11706942e-02, 7.44261996e-03,\n",
       "       5.09811114e-02, 3.08939850e-02, 9.49574545e-03, 8.51545017e-02,\n",
       "       4.35784298e-02, 2.45870885e-03, 9.47521624e-02, 2.50052658e-02,\n",
       "       3.16112857e-02, 9.07173890e-03, 5.47553566e-02, 1.33800849e-02,\n",
       "       1.94092860e-02, 5.42222727e-03, 9.53904591e-03, 1.82366379e-03,\n",
       "       1.73329251e-01, 3.30200370e-02, 5.30011940e-02, 4.58357346e-02,\n",
       "       6.99399974e-03, 3.92135194e-02, 2.43200208e-03, 7.12900239e-02,\n",
       "       2.22480176e-02, 3.45410404e-02, 1.10309220e-02, 3.30182582e-02,\n",
       "       1.23732608e-02, 9.51919691e-02, 3.82663169e-02, 4.34034260e-02,\n",
       "       1.87718837e-02, 8.05058206e-02, 4.52914036e-03, 1.11185891e-02,\n",
       "       3.38122972e-02, 5.19068779e-02, 2.47690032e-02, 1.07415385e-01,\n",
       "       1.42476964e-01, 8.25577141e-02, 4.27615435e-02, 2.23692801e-02,\n",
       "       8.33041620e-02, 1.83544926e-02, 1.05926887e-02, 1.31752879e-02,\n",
       "       1.04332857e-02, 1.64842434e-02, 1.75903134e-02, 4.37339254e-02,\n",
       "       7.59442364e-02, 2.23821830e-02, 4.61628574e-02, 6.84979030e-02,\n",
       "       1.17677174e-02, 7.89967795e-03, 8.04906830e-03, 1.18078117e-02,\n",
       "       4.44784309e-02, 2.90971442e-02, 2.84920703e-02, 3.61635984e-02,\n",
       "       1.59159718e-02, 3.06272230e-02, 2.40282072e-02, 3.41888992e-02,\n",
       "       7.67587689e-02, 7.38122770e-02, 1.14653815e-02, 2.06008946e-02,\n",
       "       4.47409040e-02, 3.42495102e-02, 4.32085126e-02, 4.32073796e-02,\n",
       "       7.36441227e-02, 5.79659663e-02, 1.06770799e-02, 4.40676539e-02,\n",
       "       5.07088898e-02, 8.05058587e-03, 3.24514911e-02, 2.13956879e-02,\n",
       "       3.56741424e-02, 5.50065640e-02, 1.53703260e-02, 1.04482535e-01,\n",
       "       3.39195510e-02, 2.05663961e-02, 2.85990233e-02, 9.33900155e-02,\n",
       "       1.20850878e-03, 1.91757852e-02, 4.08796145e-02, 1.66916047e-01,\n",
       "       1.35462546e-01, 1.34278996e-01, 1.53015314e-02, 1.39876464e-03,\n",
       "       2.29770775e-03, 6.36049677e-02, 2.54495558e-02, 1.91543129e-02,\n",
       "       1.56660048e-02, 9.01676361e-03, 1.57197143e-02, 3.24117179e-02,\n",
       "       3.02610539e-02, 6.20073808e-02, 2.89168555e-02, 7.70876909e-03,\n",
       "       1.48071772e-02, 2.94436965e-03, 3.16123590e-02, 5.39911422e-03,\n",
       "       4.73846337e-03, 4.87404938e-02, 1.67520916e-02, 9.40836142e-03,\n",
       "       5.18801631e-02, 2.84725100e-02, 2.86020299e-03, 3.57626699e-02,\n",
       "       5.41417055e-02, 2.77369449e-02, 4.37114649e-02, 8.58104651e-03,\n",
       "       1.06385306e-01, 3.77253224e-02, 1.41448081e-01, 7.19504404e-02,\n",
       "       3.49940833e-02, 1.29321593e-01, 1.68452728e-02, 3.49651908e-02,\n",
       "       5.50482441e-02, 3.30724431e-02, 8.79784332e-02, 7.08612941e-02,\n",
       "       5.60823587e-02, 2.67109767e-02, 6.73433216e-02, 5.04948586e-02,\n",
       "       4.97554732e-03, 1.36686874e-02, 2.56240270e-02, 1.18392932e-02,\n",
       "       5.19458467e-02, 8.14593634e-02, 1.20287412e-04, 1.62748463e-02,\n",
       "       3.54479093e-02, 9.33686396e-03, 3.04683150e-02, 2.37253228e-03,\n",
       "       6.60524257e-02, 1.44352303e-02, 1.19695615e-03, 4.88873152e-02,\n",
       "       4.09503961e-02, 4.20829155e-02, 7.11614916e-03, 1.44809875e-03,\n",
       "       8.28053685e-02, 9.94098064e-03, 1.54958935e-02, 1.77289455e-01,\n",
       "       4.99841957e-02, 9.16958751e-03, 1.15405131e-01, 3.34588716e-02,\n",
       "       1.76267503e-02, 5.07099591e-02, 6.09773107e-02, 3.36611661e-02,\n",
       "       8.53661615e-02, 6.21761885e-04, 6.66225079e-02, 3.45839795e-02,\n",
       "       1.61991661e-02, 2.42118755e-02, 7.74999732e-02, 1.46916883e-01,\n",
       "       2.99848505e-02, 6.69393856e-02, 1.23332025e-02, 4.34253801e-03,\n",
       "       3.62592653e-02, 6.04682874e-03, 1.13810954e-01, 1.37403416e-02,\n",
       "       1.26128741e-02, 1.38445986e-02, 4.54811579e-02, 8.75478807e-03,\n",
       "       6.67409621e-02, 6.85465824e-02, 4.65750503e-02, 7.52929519e-02,\n",
       "       1.45333673e-02, 3.24946332e-03, 2.92627678e-02, 5.20966557e-03,\n",
       "       3.28850709e-02, 2.74667550e-02, 2.03571847e-01, 2.96027186e-02,\n",
       "       7.26032029e-03, 2.35261403e-02, 7.83954893e-02, 7.67057626e-02,\n",
       "       5.44949868e-02, 4.94468845e-02, 1.78349313e-02, 2.24392917e-01,\n",
       "       8.27386912e-02, 1.22988031e-02, 1.98575268e-01, 3.93202315e-03,\n",
       "       2.09899764e-02, 5.93653463e-02, 1.38848715e-02, 1.20698766e-01,\n",
       "       9.56919971e-04, 2.57285679e-02, 8.82354588e-02, 1.54478487e-02,\n",
       "       4.89022385e-02, 9.40960257e-02, 1.62506707e-04, 1.23285635e-02,\n",
       "       1.15658693e-02, 5.11643736e-02, 9.01532312e-02, 5.19283596e-02,\n",
       "       5.13193658e-02, 3.06547949e-02, 7.82868489e-04, 4.90789058e-02,\n",
       "       3.11744368e-02, 1.41531636e-02, 7.12156152e-03, 7.83247958e-02,\n",
       "       9.95668174e-02, 6.93231716e-02, 2.24469096e-02, 4.18810424e-03,\n",
       "       1.51589344e-02, 1.74984315e-01, 7.27569254e-03, 2.59926314e-02,\n",
       "       9.10652003e-03, 2.37528000e-03, 1.34211362e-02, 1.78634868e-02,\n",
       "       2.21493630e-02, 3.79399057e-02, 1.05643354e-01, 7.08872898e-02,\n",
       "       9.39593189e-02, 6.87522221e-02, 2.29122264e-01, 8.82656729e-04,\n",
       "       1.15828078e-01, 2.41946868e-02, 7.47377386e-02, 1.77204424e-02,\n",
       "       3.38011810e-03, 1.96788460e-02, 8.13038749e-02, 6.06875603e-02,\n",
       "       4.23382524e-02, 1.23587639e-02, 5.08356080e-02, 5.19408042e-02,\n",
       "       5.66659354e-02, 9.26196621e-03, 6.16669003e-02, 1.42775893e-02,\n",
       "       1.13080378e-01, 7.93401871e-02, 2.48202918e-02, 1.32263308e-02,\n",
       "       1.54329269e-02, 1.06541914e-03, 4.11762273e-03, 1.24928383e-01,\n",
       "       3.97412966e-02, 2.86919539e-01, 6.30557892e-03, 4.98383414e-02,\n",
       "       6.79097200e-04, 1.34731043e-03, 1.25129688e-01, 3.81986938e-02])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.sqrt(sigma_n2/2)*(np.random.normal(size=K)+j*np.random.normal(size=K)) \n",
    "np.absolute(n)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0.70710678-0.70710678j 0.        +0.j         0.        +0.j\n",
      " 0.        +0.j        ]\n",
      "[-0.70710678-0.70710678j  0.70710678-0.70710678j  0.        +0.j\n",
      "  0.        +0.j        ]\n",
      "[ 0.70710678+0.70710678j -0.70710678-0.70710678j  0.70710678-0.70710678j\n",
      "  0.        +0.j        ]\n",
      "[ 0.70710678+0.70710678j  0.70710678+0.70710678j -0.70710678-0.70710678j\n",
      "  0.70710678-0.70710678j]\n",
      "[-0.70710678-0.70710678j  0.70710678+0.70710678j  0.70710678+0.70710678j\n",
      " -0.70710678-0.70710678j]\n",
      "[ 0.70710678-0.70710678j -0.70710678-0.70710678j  0.70710678+0.70710678j\n",
      "  0.70710678+0.70710678j]\n",
      "[-0.70710678-0.70710678j  0.70710678-0.70710678j -0.70710678-0.70710678j\n",
      "  0.70710678+0.70710678j]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros([N]) # input at a certain iteration (tapped delay line)\n",
    "d = np.array([]) # Desired signal\n",
    "\n",
    "for k in np.arange(8):\n",
    "    print (X)\n",
    "    X = np.append(x[k], X[0:N-1])\n",
    "    d = np.append(d, np.dot(w_o, X)+n[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in cdouble_scalars\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLMS(step=0.1, gamma=1e-12, filter_order=3)\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W = np.ones([n_ensembles, K+1, N], dtype=complex) # coefficient vector for each iteration and realization, w[0] = [1, 1, ..., 1]\n",
    "MSE = np.zeros([n_ensembles, K]) # MSE vector for each realization\n",
    "MSE_min = MSE # Minimum MSE for each realization \n",
    "\n",
    "for ensemble in np.arange(n_ensembles):\n",
    "    X = np.zeros([N]) # input at a certain iteration (tapped delay line)\n",
    "    d = np.array([]) # Desired signal\n",
    "    \n",
    "    # Creating the input signal (normalized)\n",
    "    x = (np.sign(np.random.normal(size=K)) + j*np.sign(np.random.normal(size=K)))/np.sqrt(2) \n",
    "    sigma_x2 = np.var(x) # signal power = 1\n",
    "    n = np.sqrt(sigma_n2/2)*(np.random.normal(size=K)+j*np.random.normal(size=K)) # complex noise\n",
    "    \n",
    "    for k in np.arange(K):\n",
    "        X = np.append(x[k], X[1:N])\n",
    "        d = np.append(d, np.dot(w_o, X)+n[k])\n",
    "    \n",
    "    init_coef = W[ensemble][1]\n",
    "    filter_order = N-1    \n",
    "    \n",
    "    nlms = NLMS(step=mu, filter_order=filter_order, gamma=gamma, init_coef=init_coef)\n",
    "    nlms.fit(d, x)\n",
    "\n",
    "#     W[ensemble] = nlms.coef_vector\n",
    "#     MSE[ensemble] = MSE[ensemble] + abs(nlms.error_vector)**2\n",
    "#     MSE_min[ensemble] = MSE_min[ensemble] + abs(n)**2\n",
    "\n",
    "# W_av = np.mean(W, axis=0)\n",
    "# MSE_av = np.mean(MSE, axis=0)\n",
    "# MSEmin_av = np.mean(MSE_min, axis=0)\n",
    "    \n",
    "print (nlms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(16,8), sharex=True)\n",
    "ax[0].plot(np.arange(K), 10*np.log10(MSE_av))\n",
    "ax[0].set_title('Learning Curve for MSE')\n",
    "ax[0].set_ylabel('MSE [dB]')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(np.arange(K), 10*np.log10(MSEmin_av))\n",
    "ax[1].set_title('Learning Curve for MSEmin')\n",
    "ax[1].set_ylabel('MSEmin [dB]')\n",
    "ax[1].set_xlabel('Number of iterations, k') \n",
    "ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(16,8), sharex=True)\n",
    "ax[0].plot(np.arange(K+1), np.real(W_av[:,0]))\n",
    "ax[0].set_title('Evolution of the 1st coefficient (real part)')\n",
    "ax[0].set_ylabel('Coefficient')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(np.arange(K+1), np.imag(W_av[:,0]))\n",
    "ax[1].set_title('Evolution of the 1st coefficient (imaginary part)')\n",
    "ax[1].set_ylabel('Coefficient')\n",
    "ax[1].set_xlabel('Number of iterations, k') \n",
    "ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLMS(step=1, gamma=3, filter_order=2)\n"
     ]
    }
   ],
   "source": [
    "class NLMS(LMS):\n",
    "    def __init__(self, step, filter_order, gamma, init_coef = None):                \n",
    "        LMS.__init__(self, step=step, filter_order=filter_order, init_coef=init_coef)        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"NLMS(step={}, gamma={}, filter_order={})\".format(self.step, self.gamma, self.filter_order)\n",
    "        \n",
    "    def fit(self, d, x):\n",
    "        # Pre allocations\n",
    "        self.d = np.array(d)        \n",
    "        self.x = np.array(x)        \n",
    "        self.n_iterations = len(self.d)\n",
    "        \n",
    "        # Initial State Weight Vector if passed as argument\n",
    "        self.coef_vector = np.array([self.init_coef]) if self.init_coef is not None else np.array([np.zeros([self.n_coef])])\n",
    "\n",
    "        # Improve source code regularity\n",
    "        prefixed_input = np.append(np.zeros([self.n_coef]), self.x)\n",
    "        \n",
    "        for i in np.arange(self.n_iterations):        \n",
    "            regressor = prefixed_input[i+self.n_coef:i:-1]            \n",
    "            y = np.dot(self.coef_vector[i], regressor)            \n",
    "            if i == 0:\n",
    "                self.output_vector = np.array([y])\n",
    "                self.error_vector = np.array([self.d[i]-self.output_vector[i]])\n",
    "            else:\n",
    "                self.output_vector = np.append(self.output_vector, y)\n",
    "                self.error_vector = np.append(self.error_vector, self.d[i]-self.output_vector[i])\n",
    "            \n",
    "            self.error_vector[i] = self.d[i]-self.output_vector[i]            \n",
    "            self.coef_vector = np.append(self.coef_vector, [self.step/(self.gamma+np.dot(regressor.T, regressor))*np.conj(self.error_vector[i])*regressor], axis=0)\n",
    "                        \n",
    "        return self.output_vector, self.error_vector, self.coef_vector\n",
    "    \n",
    "print (NLMS(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMS:\n",
    "    def __init__(self, step, filter_order, init_coef = None):        \n",
    "        self.step = step\n",
    "        self.filter_order = filter_order\n",
    "        self.init_coef = np.array(init_coef)\n",
    "    \n",
    "        # Initialization Procedure\n",
    "        self.n_coef = self.filter_order + 1\n",
    "        self.d = None\n",
    "        self.x = None\n",
    "        self.n_iterations = None\n",
    "        self.error_vector = None\n",
    "        self.output_vector = None\n",
    "        self.coef_vector = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"LMS(step={}, filter_order={})\".format(self.step, self.filter_order)\n",
    "        \n",
    "    def fit(self, d, x):\n",
    "        # Pre allocations\n",
    "        self.d = np.array(d)        \n",
    "        self.x = np.array(x)        \n",
    "        self.n_iterations = len(self.d)\n",
    "        \n",
    "        # Initial State Weight Vector if passed as argument\n",
    "        self.coef_vector = np.array([self.init_coef]) if self.init_coef is not None else np.array([np.zeros([self.n_coef])])\n",
    "\n",
    "        # Improve source code regularity\n",
    "        prefixed_input = np.append(np.zeros([self.n_coef]), self.x)\n",
    "        \n",
    "        for i in np.arange(self.n_iterations):        \n",
    "            regressor = prefixed_input[i+self.n_coef:i:-1]            \n",
    "            y = np.dot(self.coef_vector[i], regressor)            \n",
    "            if i == 0:\n",
    "                self.output_vector = np.array([y])\n",
    "                self.error_vector = np.array([self.d[i]-self.output_vector[i]])\n",
    "            else:\n",
    "                self.output_vector = np.append(self.output_vector, y)\n",
    "                self.error_vector = np.append(self.error_vector, self.d[i]-self.output_vector[i])\n",
    "            \n",
    "            self.error_vector[i] = self.d[i]-self.output_vector[i]            \n",
    "            self.coef_vector = np.append(self.coef_vector, [self.step*np.conj(self.error_vector[i])*regressor], axis=0)\n",
    "                        \n",
    "        return self.output_vector, self.error_vector, self.coef_vector\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Makes predictions for a new signal after weights are fit\n",
    "        return np.dot(self.coef_vector[-1], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
